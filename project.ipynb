{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5af8fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bac346e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK data...\n",
      "Downloads complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dviss\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dviss\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "print(\"Downloading NLTK data...\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "print(\"Downloads complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9df9780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Dataset Creation ---\n",
      "Successfully loaded 'comments.csv' and 'train.csv'.\n",
      "Remapping existing labels...\n",
      "Adding 'threat' category...\n",
      "Creating 'constructive_criticism' category...\n",
      "Creating 'spam_irrelevant' category...\n",
      "Combining all datasets...\n",
      "\n",
      "--- SUCCESS! Your 7 Categories Are Ready ---\n",
      "category\n",
      "emotional                 183871\n",
      "praise                    141067\n",
      "hate_abuse                 57317\n",
      "support                    34554\n",
      "threat                       478\n",
      "constructive_criticism        60\n",
      "spam_irrelevant               40\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "print(\"--- Starting Dataset Creation (v3, with 7-Category Augmentation) ---\")\n",
    "\n",
    "# --- 1. Load Source Data ---\n",
    "try:\n",
    "    df_existing = pd.read_csv(\"comments.csv\")\n",
    "    df_toxic_source = pd.read_csv(\"train.csv\")\n",
    "    print(\"Successfully loaded 'comments.csv' and 'train.csv'.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Make sure 'comments.csv' and 'train.csv' are in the same folder.\")\n",
    "\n",
    "# --- 2. Map Your 0-6 Labels to the New Categories ---\n",
    "def map_label_to_category(label):\n",
    "    if label == 1:  # 1:joy\n",
    "        return 'praise'\n",
    "    if label == 2:  # 2:love\n",
    "        return 'support'\n",
    "    if label in [3, 6]:  # 3:anger, 6:toxic\n",
    "        return 'hate_abuse'\n",
    "    if label in [0, 4, 5]:  # 0:sadness, 4:fear, 5:surprise\n",
    "        return 'emotional'\n",
    "    return None\n",
    "\n",
    "print(\"Remapping existing labels...\")\n",
    "df_remapped = pd.DataFrame()\n",
    "df_remapped['text'] = df_existing['text']\n",
    "df_remapped['category'] = df_existing['label'].apply(map_label_to_category)\n",
    "df_remapped = df_remapped.dropna(subset=['category'])\n",
    "\n",
    "# --- 3. Add Missing Categories & Augment ALL Categories ---\n",
    "\n",
    "# Category 1: Threat (from train.csv)\n",
    "df_threat = df_toxic_source[df_toxic_source['threat'] == 1].copy()\n",
    "df_threat_to_add = pd.DataFrame({'text': df_threat['comment_text'], 'category': 'threat'})\n",
    "\n",
    "# --- !! NEW !! Using your high-quality examples ---\n",
    "\n",
    "# 1. Praise\n",
    "praise_texts = [\n",
    "    \"This is brilliant! You explained a complex topic so simply.\",\n",
    "    \"Flawless execution. That was incredibly well done.\",\n",
    "    \"Best video I've seen this week. Subscribed!\"\n",
    "] * 20\n",
    "df_praise_to_add = pd.DataFrame({'text': praise_texts, 'category': 'praise'})\n",
    "\n",
    "# 2. Support\n",
    "support_texts = [\n",
    "    \"Don't let the negative comments get to you. This is valuable content.\",\n",
    "    \"I really appreciate the amount of work you must have put into this.\",\n",
    "    \"So excited to see your next video!\"\n",
    "] * 20\n",
    "df_support_to_add = pd.DataFrame({'text': support_texts, 'category': 'support'})\n",
    "\n",
    "# 3. Constructive Criticism\n",
    "crit_texts = [\n",
    "    \"Great video, but the audio quality was a bit echoey. A different mic might help.\",\n",
    "    \"I liked the overall point, but the pacing felt a bit slow in the middle.\",\n",
    "    \"The code is good, but you could make it more efficient by using a different data structure for that part.\"\n",
    "] * 20\n",
    "df_crit_to_add = pd.DataFrame({'text': crit_texts, 'category': 'constructive_criticism'})\n",
    "\n",
    "# 4. Hate/Abuse\n",
    "hate_texts = [\n",
    "    \"You have no idea what you're talking about.\",\n",
    "    \"This is the stupidest thing I've ever read.\",\n",
    "    \"Just delete your channel already.\"\n",
    "] * 20\n",
    "df_hate_to_add = pd.DataFrame({'text': hate_texts, 'category': 'hate_abuse'})\n",
    "\n",
    "# 5. Threat (Augmenting the ones from train.csv)\n",
    "threat_texts = [\n",
    "    \"If you don't take this down, I'm going to flag all your videos.\",\n",
    "    \"I've screenshotted this and I'm sending it to your boss.\",\n",
    "    \"You're going to regret posting this.\"\n",
    "] * 20\n",
    "df_threat_aug_to_add = pd.DataFrame({'text': threat_texts, 'category': 'threat'})\n",
    "\n",
    "# 6. Emotional\n",
    "emo_texts = [\n",
    "    \"This actually made me tear up a bit. So powerful.\",\n",
    "    \"I was having a really bad day, and this made me laugh out loud. Thank you.\",\n",
    "    \"Wow, this is deeply moving. It really makes you think.\"\n",
    "] * 20\n",
    "df_emo_to_add = pd.DataFrame({'text': emo_texts, 'category': 'emotional'})\n",
    "\n",
    "# 7. Irrelevant/Spam\n",
    "spam_texts = [\n",
    "    \"Check out my profile for amazing deals!\",\n",
    "    \"First!\",\n",
    "    \"www. buy-this-scam. com\",\n",
    "    \"sub for sub? I subscribed to you.\"\n",
    "] * 20\n",
    "df_spam_to_add = pd.DataFrame({'text': spam_texts, 'category': 'spam_irrelevant'})\n",
    "\n",
    "# (We'll also keep the question category from before)\n",
    "question_texts = [\n",
    "    \"What software did you use to make this?\",\n",
    "    \"Can you make a video on topic X next?\",\n",
    "] * 20\n",
    "df_question_to_add = pd.DataFrame({'text': question_texts, 'category': 'question_suggestion'})\n",
    "\n",
    "\n",
    "# --- 4. Combine Everything into One DataFrame ---\n",
    "print(\"Combining all datasets...\")\n",
    "df = pd.concat([\n",
    "    df_remapped,            # Your main 400k+ rows\n",
    "    df_threat_to_add,       # Original threat data\n",
    "    df_praise_to_add,       # Your new praise examples\n",
    "    df_support_to_add,      # Your new support examples\n",
    "    df_crit_to_add,         # Your new constructive criticism examples\n",
    "    df_hate_to_add,         # Your new hate examples\n",
    "    df_threat_aug_to_add,   # Your new threat examples\n",
    "    df_emo_to_add,          # Your new emotional examples\n",
    "    df_spam_to_add,         # Your new spam examples\n",
    "    df_question_to_add      # The question examples\n",
    "], ignore_index=True)\n",
    "\n",
    "# --- 5. VERIFY: Check Your New Categories ---\n",
    "print(\"\\n--- SUCCESS! Your model is now training on your new examples ---\")\n",
    "print(df['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1444c087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting text preprocessing...\n",
      "Preprocessing complete. 'cleaned_text' column is now in the DataFrame.\n"
     ]
    }
   ],
   "source": [
    "# --- Define preprocessing components ---\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text) # Remove punctuation/numbers\n",
    "    tokens = text.split()               # Use simple .split()\n",
    "    \n",
    "    # Lemmatize and remove stopwords\n",
    "    cleaned_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    \n",
    "    return \" \".join(cleaned_tokens)\n",
    "\n",
    "# --- Apply the preprocessing ---\n",
    "print(\"Starting text preprocessing...\")\n",
    "df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "print(\"Preprocessing complete. 'cleaned_text' column is now in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa25a244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining X and y...\n",
      "Splitting data...\n",
      "Vectorizing text...\n",
      "Training the model...\n",
      "Model training complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"Defining X and y...\")\n",
    "# This will now work, because 'df' has the correct columns\n",
    "X = df['cleaned_text']\n",
    "y = df['category']\n",
    "\n",
    "# Split the data\n",
    "print(\"Splitting data...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Vectorize\n",
    "print(\"Vectorizing text...\")\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train\n",
    "print(\"Training the model...\")\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c491f352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model...\n",
      "\n",
      "--- Classification Report ---\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "constructive_criticism       1.00      1.00      1.00        12\n",
      "             emotional       0.95      0.96      0.95     36774\n",
      "            hate_abuse       0.91      0.90      0.90     11463\n",
      "                praise       0.92      0.93      0.93     28214\n",
      "       spam_irrelevant       1.00      1.00      1.00         8\n",
      "               support       0.81      0.77      0.79      6911\n",
      "                threat       1.00      0.46      0.63        96\n",
      "\n",
      "              accuracy                           0.92     83478\n",
      "             macro avg       0.94      0.86      0.89     83478\n",
      "          weighted avg       0.92      0.92      0.92     83478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating the model...\")\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "print(\"\\n--- Classification Report ---\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cdec9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool functions 'classify_comment' and 'get_reply_suggestion' are ready.\n"
     ]
    }
   ],
   "source": [
    "# These functions use the 'model' and 'vectorizer' from Cell 5\n",
    "def classify_comment(comment):\n",
    "    cleaned_comment = preprocess_text(comment)\n",
    "    comment_tfidf = vectorizer.transform([cleaned_comment])\n",
    "    prediction = model.predict(comment_tfidf)\n",
    "    return prediction[0]\n",
    "\n",
    "def get_reply_suggestion(category):\n",
    "    templates = {\n",
    "        'praise_support': \"Thank you so much for the kind words!\",\n",
    "        'hate_abuse': \"[Action: Monitor user or escalate to moderation.]\",\n",
    "        'emotional': \"Thank you for sharing that with us. It means a lot.\",\n",
    "        'threat': \"[Action: Escalate to security/legal team immediately.]\",\n",
    "        'constructive_criticism': \"That's valuable feedback. We'll pass it to the team.\",\n",
    "        'spam_irrelevant': \"[Action: Remove comment and monitor user.]\",\n",
    "        'question_suggestion': \"That's a great question! We'll look into it.\"\n",
    "    }\n",
    "    return templates.get(category, \"No suggestion available.\")\n",
    "\n",
    "print(\"Tool functions 'classify_comment' and 'get_reply_suggestion' are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c78ac24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment: 'I loved the video, but the audio was really hard to hear.'\n",
      "Category: support\n",
      "Suggested Reply: No suggestion available.\n",
      "\n",
      "Comment: 'This is amazing! You guys are the best!'\n",
      "Category: emotional\n",
      "Suggested Reply: Thank you for sharing that with us. It means a lot.\n",
      "\n",
      "Comment: 'I'll report you if this continues.'\n",
      "Category: emotional\n",
      "Suggested Reply: Thank you for sharing that with us. It means a lot.\n",
      "\n",
      "Comment: 'check out my website www.buy-stuff.com'\n",
      "Category: emotional\n",
      "Suggested Reply: Thank you for sharing that with us. It means a lot.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Constructive\n",
    "test_comment_1 = \"I loved the video, but the audio was really hard to hear.\"\n",
    "category_1 = classify_comment(test_comment_1)\n",
    "print(f\"Comment: '{test_comment_1}'\")\n",
    "print(f\"Category: {category_1}\")\n",
    "print(f\"Suggested Reply: {get_reply_suggestion(category_1)}\\n\")\n",
    "\n",
    "# Test 2: Praise\n",
    "test_comment_2 = \"This is amazing! You guys are the best!\"\n",
    "category_2 = classify_comment(test_comment_2)\n",
    "print(f\"Comment: '{test_comment_2}'\")\n",
    "print(f\"Category: {category_2}\")\n",
    "print(f\"Suggested Reply: {get_reply_suggestion(category_2)}\\n\")\n",
    "\n",
    "# Test 3: Threat\n",
    "test_comment_3 = \"I'll report you if this continues.\"\n",
    "category_3 = classify_comment(test_comment_3)\n",
    "print(f\"Comment: '{test_comment_3}'\")\n",
    "print(f\"Category: {category_3}\")\n",
    "print(f\"Suggested Reply: {get_reply_suggestion(category_3)}\\n\")\n",
    "\n",
    "# Test 4: Spam\n",
    "test_comment_4 = \"check out my website www.buy-stuff.com\"\n",
    "category_4 = classify_comment(test_comment_4)\n",
    "print(f\"Comment: '{test_comment_4}'\")\n",
    "print(f\"Category: {category_4}\")\n",
    "print(f\"Suggested Reply: {get_reply_suggestion(category_4)}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
